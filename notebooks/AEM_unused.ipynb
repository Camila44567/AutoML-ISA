{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have unbalanced classes in df_performance, let's use k-medoids on the majority class with k = size of the minority class. We keep each medoid as a way to balance the classes in the train datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################################################################\n",
    "\n",
    "# This function is used to extract intervals of Good and Bad instances\n",
    "# for each dataset using each performance list balanced with kmedoids\n",
    "def auto_extraction_balanced(list_df_data, list_df_metadata, list_df_performance):\n",
    "    \n",
    "    # Let's define the good and bad interval dictionary\n",
    "    dict_G = {}\n",
    "    dict_B = {}\n",
    "\n",
    "    # We'll be evaluating the domains of competence of each performance measure within the dataset\n",
    "    \n",
    "    # Let's go through each class (performance)\n",
    "    for list_index in range(len(list_df_performance)):\n",
    "        \n",
    "        # Let's get the current performance name\n",
    "        performance = list(list_df_performance[list_index].columns)[0]\n",
    "        \n",
    "        # Let's create an empty dictionary for this performance measure\n",
    "        dict_G[performance] = {}\n",
    "        dict_B[performance] = {}\n",
    "\n",
    "        # Let's get the meta feature names for the current dataset\n",
    "        meta_feature_names = list(list_df_metadata[list_index].columns)\n",
    "\n",
    "        # let's go through each column of the meta features (MFj)\n",
    "        for metafeature in meta_feature_names:\n",
    "            \n",
    "            # Let's create an empty dictionary for this metafeature \n",
    "            # in this algorithm\n",
    "            dict_G[performance][metafeature] = {}\n",
    "            dict_B[performance][metafeature] = {}\n",
    "            \n",
    "            # Get interval values and indexes\n",
    "            G_int_ind, G_int_val, B_int_ind, B_int_val = get_intervals(list_df_data[list_index], list_df_metadata[list_index][metafeature], list_df_performance[list_index])\n",
    "            \n",
    "            # Add G_aux and B_aux to dictionaries according to the current \n",
    "            # algorithm, metafeature and interval        \n",
    "            dict_G[performance][metafeature]['interval_ind'] = G_int_ind\n",
    "            dict_G[performance][metafeature]['interval_val'] = G_int_val\n",
    "            dict_B[performance][metafeature]['interval_ind'] = B_int_ind\n",
    "            dict_B[performance][metafeature]['interval_val'] = B_int_val\n",
    "        \n",
    "    return dict_G, dict_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import preprocess as pp\n",
    "\n",
    "list_df_train_balanced = []\n",
    "list_df_metadata_train_balanced = []\n",
    "list_df_performance_train_balanced = []\n",
    "\n",
    "# for each performance measure:\n",
    "for column in df_performance_train:\n",
    "    \n",
    "    #data, metadata, performance  = pp.kmedoids_balance(column, df_performance_train, df_metadata_train, df_train)\n",
    "    \n",
    "    data, metadata, performance  = pp.random_balance(column, df_performance_train, df_metadata_train, df_train)\n",
    "\n",
    "    list_df_train_balanced.append(data)\n",
    "    list_df_metadata_train_balanced.append(metadata)\n",
    "    list_df_performance_train_balanced.append(performance)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_G, dict_B, = aem.auto_extraction_balanced(list_df_train_balanced, \n",
    "#                                      list_df_metadata_train_balanced, \n",
    "#                                      list_df_performance_train_balanced)\n",
    "\n",
    "# visualize the tables of intervals obtained by the AEM for each performance measure\n",
    "# for performance in dict_good_rules:\n",
    "#    print(f\"{performance}\")\n",
    "#    aem.display_side_by_side(dict_good_rules[performance] ,dict_bad_rules[performance], titles=['Good','Bad'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cf_matrix as cm\n",
    "\n",
    "# Let's get the performance names for the current dataset\n",
    "performance_names = list(df_performance_test.columns)\n",
    "\n",
    "for performance in performance_names:\n",
    "    dict_pred[performance].PRD = dict_pred[performance].PRD == 1\n",
    "    dict_pred[performance].NRD = dict_pred[performance].NRD == 1\n",
    "    \n",
    "    dict_pred[performance]['PRD_not_NRD'] = dict_pred[performance].PRD & ~dict_pred[performance].NRD\n",
    "    dict_pred[performance]['NRD_not_PRD'] = ~dict_pred[performance].PRD & dict_pred[performance].NRD\n",
    "    \n",
    "    # Which rules are used to classify Good and Bad instances\n",
    "    mask1 = dict_pred[performance].PRD_not_NRD == True\n",
    "    mask2 = dict_pred[performance].NRD_not_PRD == True\n",
    "    \n",
    "    dict_pred[performance]['Final'] = [-1] * len(df_metadata_test)\n",
    "    \n",
    "    dict_pred[performance].loc[mask2, ('Final')] = 0\n",
    "    dict_pred[performance].loc[mask1, ('Final')] = 1\n",
    "\n",
    "for performance in performance_names:\n",
    "    matrix = confusion_matrix(df_performance_test[performance], dict_pred[performance].Final)\n",
    "    labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    #categories = ['Bad', 'Good']\n",
    "    categories = ['None', 'Bad', 'Good']\n",
    "    cm.make_confusion_matrix(matrix, \n",
    "                      group_names=labels,\n",
    "                      categories=categories, \n",
    "                      cmap='Blues',\n",
    "                      title = performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to extract intervals of original features of Good and Bad instances\n",
    "# for each dataset using each performance column\n",
    "def auto_extraction_original(df_data, df_performance, percent_drop, percent_merge):\n",
    "    \n",
    "    # Let's define the good and bad interval dictionary\n",
    "    dict_G = {}\n",
    "    dict_B = {}\n",
    "\n",
    "    # We'll be evaluating the domains of competence of each performance measure within the dataset\n",
    "    # Let's get the performance names for the current dataset\n",
    "    performance_names = list(df_performance.columns)\n",
    "    \n",
    "    # Let's go through each class (performance)\n",
    "    for performance in performance_names:\n",
    "\n",
    "        # Let's create an empty dictionary for this performance measure\n",
    "        dict_G[performance] = {}\n",
    "        dict_B[performance] = {}\n",
    "\n",
    "        # Let's get the original feature names for the current dataset\n",
    "        original_feature_names = list(df_data.columns)\n",
    "\n",
    "        # let's go through each column of the original features (OFj)\n",
    "        for feature in original_feature_names:\n",
    "            \n",
    "            # Let's create an empty dictionary for this feature \n",
    "            # in this algorithm\n",
    "            dict_G[performance][feature] = {}\n",
    "            dict_B[performance][feature] = {}\n",
    "            \n",
    "            # Get interval values and indexes\n",
    "            if isinstance(percent_drop, pd.Series):\n",
    "                    G_int_ind, G_int_val, B_int_ind, B_int_val = get_intervals(df_data, df_data[feature], df_performance[performance], percent_drop[performance], percent_merge[performance])\n",
    "                    \n",
    "            else: \n",
    "                G_int_ind, G_int_val, B_int_ind, B_int_val = get_intervals(df_data, df_data[feature], df_performance[performance], percent_drop, percent_merge)\n",
    "            \n",
    "            # Add G_aux and B_aux to dictionaries according to the current \n",
    "            # algorithm, metafeature and interval        \n",
    "            dict_G[performance][feature]['interval_ind'] = G_int_ind\n",
    "            dict_G[performance][feature]['interval_val'] = G_int_val\n",
    "            dict_B[performance][feature]['interval_ind'] = B_int_ind\n",
    "            dict_B[performance][feature]['interval_val'] = B_int_val\n",
    "    \n",
    "    \n",
    "    return dict_G, dict_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to lay tables side-by-side in Jupyter-Notebook\n",
    "def display_side_by_side(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h2>{title}</h2>'\n",
    "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
    "        html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################################################################################################################\n",
    "\n",
    "# This function returns the support for PRD and NRD\n",
    "def rd_support(dict_good_rules, dict_bad_rules, n):\n",
    "\n",
    "    for performance in dict_good_rules:\n",
    "        print(f\"{performance}\")\n",
    "\n",
    "        df_good = dict_good_rules[performance].sort_values('Support', ascending=False)\n",
    "        df_bad = dict_bad_rules[performance].sort_values('Support', ascending=False)\n",
    "\n",
    "        # Turn to list of intervals with indexes\n",
    "        list_int_ind_good = list(df_good[\"Index\"])\n",
    "        list_int_ind_bad = list(df_bad[\"Index\"])\n",
    "\n",
    "        list_intervals_good = []\n",
    "        list_intervals_bad = []\n",
    "\n",
    "        # Positive Rule Disjunction (PRD)\n",
    "        for int_ind in list_int_ind_good:\n",
    "            list_intervals_good.append(list(range(int_ind[0], int_ind[1])))\n",
    "\n",
    "        # Negative Rule Disjunction (NRD)\n",
    "        for int_ind in list_int_ind_bad:\n",
    "            list_intervals_bad.append(list(range(int_ind[0], int_ind[1])))\n",
    "\n",
    "\n",
    "        # Union of intervals\n",
    "        final_list_good = list(set().union(*list_intervals_good))\n",
    "        final_list_bad = list(set().union(*list_intervals_bad))\n",
    "\n",
    "        support_good = round((len(final_list_good)/n), 2)\n",
    "        support_bad = round((len(final_list_bad)/n), 2)\n",
    "\n",
    "        print(\"Good: \", support_good, \"Bad: \", support_bad, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmedoids_balance(column, df_performance_train, df_metadata_train, df_train):\n",
    "    \n",
    "    # find out what is the minority and majority class\n",
    "    count = df_performance_train[column].value_counts()\n",
    "    majority_class = count[count == max(count)].index[0]\n",
    "    minority_class = count[count == min(count)].index[0]\n",
    "\n",
    "    # separate a dataframe with only the majority class and get its indexes\n",
    "    df_performance_train_majority = df_performance_train[column][df_performance_train[column] ==  majority_class]\n",
    "    train_majority_index = np.array(df_performance_train_majority.index)\n",
    "\n",
    "    # separate a dataframe with only the minority class and get its indexes\n",
    "    df_performance_train_minority = df_performance_train[column][df_performance_train[column] ==  minority_class]\n",
    "    train_minority_index = np.array(df_performance_train_minority.index)\n",
    "\n",
    "    # use the majority and minority indexes to separate the metadata train set\n",
    "    df_metadata_train_majority = df_metadata_train.iloc[train_majority_index]\n",
    "    df_metadata_train_minority = df_metadata_train.iloc[train_minority_index]\n",
    "\n",
    "    # get the cluster centers (medoids) for the majority class\n",
    "    kmedoids = KMedoids(n_clusters=count[minority_class], random_state=0).fit(df_metadata_train_majority.to_numpy())\n",
    "\n",
    "    # get the indexes for minority and medoids\n",
    "    medoids_indexes = np.array(df_metadata_train_majority.iloc[kmedoids.medoid_indices_].index)\n",
    "    index_balanced = np.concatenate((medoids_indexes, np.array(df_metadata_train_minority.index)))\n",
    "\n",
    "    # separate data, metadata and performance with index for the balanced set\n",
    "    df_train_balanced = df_train.loc[index_balanced].sort_index()\n",
    "    df_metadata_train_balanced = df_metadata_train.loc[index_balanced].sort_index()\n",
    "    df_performance_train_balanced = pd.DataFrame(df_performance_train.loc[index_balanced, column].sort_index(), columns = [column])\n",
    "\n",
    "    return df_train_balanced, df_metadata_train_balanced, df_performance_train_balanced\n",
    "\n",
    "############################################################################################\n",
    "    \n",
    "def random_balance(column, df_performance_train, df_metadata_train, df_train):\n",
    "    \n",
    "    # find out what is the minority and majority class\n",
    "    count = df_performance_train[column].value_counts()\n",
    "    majority_class = count[count == max(count)].index[0]\n",
    "    minority_class = count[count == min(count)].index[0]\n",
    "\n",
    "    # separate a dataframe with only the majority class and get its indexes\n",
    "    df_performance_train_majority = df_performance_train[column][df_performance_train[column] ==  majority_class]\n",
    "    train_majority_index = np.array(df_performance_train_majority.index)\n",
    "\n",
    "    # separate a dataframe with only the minority class and get its indexes\n",
    "    df_performance_train_minority = df_performance_train[column][df_performance_train[column] ==  minority_class]\n",
    "    train_minority_index = np.array(df_performance_train_minority.index)\n",
    "\n",
    "    # use the majority and minority indexes to separate the metadata train set\n",
    "    df_metadata_train_majority = df_metadata_train.iloc[train_majority_index]\n",
    "    df_metadata_train_minority = df_metadata_train.iloc[train_minority_index]\n",
    "    \n",
    "    # get the indexes for the majority class random sample and minority class\n",
    "    \n",
    "    # randomly sample majority class with the size of minority class\n",
    "    majority_sample_indexes = np.array(random.sample(list(train_majority_index), count[minority_class]))\n",
    "    index_balanced = np.concatenate((majority_sample_indexes, np.array(df_metadata_train_minority.index)))\n",
    "\n",
    "    # separate data, metadata and performance with index for the balanced set\n",
    "    df_train_balanced = df_train.loc[index_balanced].sort_index()\n",
    "    df_metadata_train_balanced = df_metadata_train.loc[index_balanced].sort_index()\n",
    "    df_performance_train_balanced = pd.DataFrame(df_performance_train.loc[index_balanced, column].sort_index(), columns = [column])\n",
    "\n",
    "    return df_train_balanced, df_metadata_train_balanced, df_performance_train_balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of the ruleset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import cf_matrix as cm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "# Change plot font size and update to save as latex\n",
    "#matplotlib.use(\"png\")\n",
    "plt.rcParams.update({\n",
    "    'font.size': 13\n",
    "})\n",
    "\n",
    "\n",
    "# Let's get the performance names for the current dataset\n",
    "#performance_names = list(df_performance_test.columns)\n",
    "\n",
    "performance = \"IsBetaEasy\"\n",
    "\n",
    "dict_pred[performance].PRD = dict_pred[performance].PRD == 1\n",
    "dict_pred[performance].NRD = dict_pred[performance].NRD == 1\n",
    "\n",
    "dict_pred[performance]['PRD_not_NRD'] = dict_pred[performance].PRD & ~dict_pred[performance].NRD\n",
    "dict_pred[performance]['NRD_not_PRD'] = ~dict_pred[performance].PRD & dict_pred[performance].NRD\n",
    "\n",
    "# Which rules are used to classify easy and hard instances\n",
    "mask1 = dict_pred[performance].PRD_not_NRD == True\n",
    "mask2 = dict_pred[performance].NRD_not_PRD == True\n",
    "\n",
    "dict_pred[performance]['Final'] = [-1] * len(df_metadata_train)\n",
    "\n",
    "dict_pred[performance].loc[mask2, ('Final')] = 0\n",
    "dict_pred[performance].loc[mask1, ('Final')] = 1\n",
    "\n",
    "matrix = confusion_matrix(df_performance_train[performance], dict_pred[performance].Final)\n",
    "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
    "categories = ['None', 'Hard', 'Easy']\n",
    "    \n",
    "# Get direc name\n",
    "direc_name = os.path.basename(direc)\n",
    "    \n",
    "cm.make_confusion_matrix(matrix, \n",
    "                  group_names=labels,\n",
    "                  categories=categories, \n",
    "                  cmap='Blues',\n",
    "                  sum_stats=False\n",
    "                        )\n",
    "\n",
    "# f1 score\n",
    "f1 = f1_score(df_performance_train[\"IsBetaEasy\"], \n",
    "               dict_pred[\"IsBetaEasy\"].Final, \n",
    "               average='weighted')\n",
    "\n",
    "print(f\"Weighted F1 Score Train: {round(df_best.loc['f1', 'IsBetaEasy'], 2)}\")\n",
    "print(f\"Weighted F1 Score Test: {round(f1, 2)}\")\n",
    "\n",
    "plt.savefig(f'{direc}/Images/performance_{\"IsBetaEasy\"}_{direc_name}.pgf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
